{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic TF Model & Processing Structure\n",
    "- Config class with parameters\n",
    "- Model class with methods for inference, train, evaluation\n",
    "    - using lazy_property decorator\n",
    "    \n",
    "#### Goal: RNN for learning a simple real-valued function\n",
    "\n",
    "#### Open questions\n",
    "- How do Flags fit into the picture?\n",
    "- and main.app()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "HOME = os.environ['HOME']\n",
    "os.chdir(HOME+\"/ninja/mxn/src/MxnVentures\")\n",
    "from mxn_ventures import utils\n",
    "\n",
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def rnn_data(data, time_steps, labels=False):\n",
    "    \"\"\"\n",
    "    creates new data frame based on previous observation\n",
    "      * example:\n",
    "        l = [1, 2, 3, 4, 5]\n",
    "        time_steps = 2\n",
    "        -> labels == False [[1, 2], [2, 3], [3, 4]]\n",
    "        -> labels == True [2, 3, 4, 5]\n",
    "    \"\"\"\n",
    "    rnn_df = []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        if labels:\n",
    "            try:\n",
    "                rnn_df.append(data.iloc[i + time_steps].as_matrix())\n",
    "            except AttributeError:\n",
    "                rnn_df.append(data.iloc[i + time_steps])\n",
    "        else:\n",
    "            data_ = data.iloc[i: i + time_steps].as_matrix()\n",
    "            rnn_df.append(data_ if len(data_.shape) > 1 else [[i] for i in data_])\n",
    "    return np.array(rnn_df)\n",
    "\n",
    "\n",
    "def split_data(data, val_size=0.1, test_size=0.1):\n",
    "    \"\"\"\n",
    "    splits data to training, validation and testing parts\n",
    "    \"\"\"\n",
    "    ntest = int(round(len(data) * (1 - test_size)))\n",
    "    nval = int(round(len(data.iloc[:ntest]) * (1 - val_size)))\n",
    "\n",
    "    df_train, df_val, df_test = data.iloc[:nval], data.iloc[nval:ntest], data.iloc[ntest:]\n",
    "\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "\n",
    "def prepare_data(data, time_steps, labels=False, val_size=0.1, test_size=0.1):\n",
    "    \"\"\"\n",
    "    Given the number of `time_steps` and some data,\n",
    "    prepares training, validation and test data for an lstm cell.\n",
    "    \"\"\"\n",
    "    df_train, df_val, df_test = split_data(data, val_size, test_size)\n",
    "    return (rnn_data(df_train, time_steps, labels=labels),\n",
    "            rnn_data(df_val, time_steps, labels=labels),\n",
    "            rnn_data(df_test, time_steps, labels=labels))\n",
    "\n",
    "def generate_data(fct, x, time_steps, seperate=False):\n",
    "    \"\"\"generates data with based on a function fct\"\"\"\n",
    "    data = fct(x)\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        data = pd.DataFrame(data)\n",
    "    train_x, val_x, test_x = prepare_data(data['a'] if seperate else data, time_steps)\n",
    "    train_y, val_y, test_y = prepare_data(data['b'] if seperate else data, time_steps, labels=True)\n",
    "    return dict(train=train_x, val=val_x, test=test_x), dict(train=train_y, val=val_y, test=test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class OrigConfig(object):\n",
    "    \"\"\"Original config.\"\"\"\n",
    "    layers = [100]      # n_layers = 1, hidden_size = 100\n",
    "    n_epochs = 10       # max_max_epoch = 13\n",
    "    batch_size = 50\n",
    "    # probability of dropout of hidden units\n",
    "    dropout_p_hidden = 0.5 # 0.4\n",
    "    learning_rate = 0.01   # 0.05  # lr\n",
    "    # momentum: if 0, Nesterov momentum is applied during training\n",
    "    momentum = 0.0\n",
    "    # learning rate adaptation strategy\n",
    "    adapt = 'adagrad'\n",
    "    # decay parameter for RMSProp; has no effect in other modes\n",
    "    decay = 0.9\n",
    "    # clip gradients above this value; 0 means no clippling\n",
    "    grad_cap = 0\n",
    "    # initialization width: either stdev, or min/max of the init interval;\n",
    "    # 0 means adaptive normalization (sigma depends on weigth matrix size)\n",
    "    sigma = 0\n",
    "    # either using normal or uniform distr. for weight initialization\n",
    "    init_as_normal = False\n",
    "    # loss function (top1, bpr, cross-entropy=CE)¯˘\n",
    "    loss = 'top1'\n",
    "    # activation function for hidden cells (tanh, relu)\n",
    "    hidden_act = 'tanh'\n",
    "    # final layer activation function \n",
    "    # (CE: softmax; top1/bpr: tanh /relu/linear)\n",
    "    final_act = None\n",
    "    # either to randomize sessions order in each epoch\n",
    "    train_random_order = False\n",
    "    # lambda coefficient of the L2 regularization\n",
    "    lmbd = 0.0\n",
    "    # RNN: either to reset hidden state to zero after a session finished\n",
    "    reset_after_session = True    \n",
    "    # data column names (in header)\n",
    "    session_key = 'SessionId'\n",
    "    item_key = 'ItemId'\n",
    "    time_key = 'Time'\n",
    "    \n",
    "    num_steps = 20 # from tf.rnn - what is it exactly?\n",
    "    cell_type = 'gru' # gru, lstm\n",
    "    forget_bias = 0.0\n",
    "    \n",
    "    log_dir = HOME + \"/data/tf_tut/logs/s1\"\n",
    "    time_steps = 3\n",
    "    rnn_layers = [{'num_units': 5}]\n",
    "    dense_layers = None\n",
    "    training_steps = 10000\n",
    "    print_interval = 1000 # training_steps / 10\n",
    "    batch_size = 100\n",
    "    dtype = tf.float32\n",
    "    \n",
    "    def __init__(sefl):\n",
    "        pass\n",
    "    def info(self):\n",
    "        utils.print_parameters(self)\n",
    "\n",
    "class SmallConfig(OrigConfig):\n",
    "    \"\"\"Small config.\"\"\"\n",
    "    layers = [5]\n",
    "    n_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model1(object):\n",
    "    # based on Tut2 PTBModel\n",
    "    def __init__(self, config):\n",
    "        self.batch_size = config.batch_size\n",
    "        lstm_layers = []\n",
    "        for layer in config.rnn_layers:\n",
    "            num_neurons = layer['num_units']\n",
    "            cell = tf.nn.rnn_cell.BasicLSTMCell(num_neurons, state_is_tuple=True) \n",
    "            if 'keep_prob' in layer:\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, layer['keep_prob'])\n",
    "            lstm_layers.append(layer)\n",
    "        lstm_net = tf.nn.rnn_cell.MultiRNNCell(lstm_layers, state_is_tuple=True)\n",
    "\n",
    "        self.data = tf.placeholder(config.dtype, [self.batch_size, config.time_steps])\n",
    "        self.target = tf.placeholder(config.dtype, [self.batch_size, config.time_steps])\n",
    "\n",
    "        output, state = tf.nn.dynamic_rnn(lstm_net, self.data, dtype=config.dtype)\n",
    "        \n",
    "        prediction, loss = tf.contrib.learn.models.linear_regression(output, self.target)\n",
    "        train_op = tf.contrib.layers.optimize_loss(loss, tf.contrib.layers.optimize_loss(\n",
    "            loss, tf.contrib.framework.get_global_step(), optimizer=optimizer, learning_rate=learning_rate))\n",
    "\n",
    "        \n",
    "        self._input_data = tf.placeholder(tf.int32, [self.batch_size, self.num_steps])\n",
    "        self._targets = tf.placeholder(tf.int32, [self.batch_size, self.num_steps])\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(size, forget_bias=0.0, state_is_tuple=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, config, data, target):\n",
    "        self.config = config\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.prediction\n",
    "        self.optimize\n",
    "        self.error\n",
    "    @utils.lazy_property\n",
    "    def prediction(self):\n",
    "        data_size = int(self.data.get_shape()[1])\n",
    "        target_size = int(self.target.get_shape()[1])\n",
    "        weight = tf.Variable(tf.truncated_normal([data_size, target_size]))\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[target_size]))\n",
    "        incoming = tf.matmul(self.data, weight) + bias\n",
    "        return tf.nn.softmax(incoming)\n",
    "    @utils.lazy_property\n",
    "    def optimize(self):\n",
    "        cross_entropy = -tf.reduce_sum(self.target, tf.log(self.prediction))\n",
    "        optimizer = tf.train.RMSPropOptimizer(0.03)\n",
    "        return optimizer.minimize(cross_entropy)\n",
    "    @utils.lazy_property\n",
    "    def error(self):\n",
    "        mistakes = tf.not_equal(\n",
    "            tf.argmax(self.target, 1), tf.argmax(self.prediction, 1))\n",
    "        return tf.reduce_mean(tf.cast(mistakes, tf.float32))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
